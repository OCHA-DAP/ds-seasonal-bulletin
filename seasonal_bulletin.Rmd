---
output:
  html_document
params:
  iso3: SOM
  adm_level: 1
  dataset: seas5
  season: MAM
  issued_month: 2
  year: 2025
---

# `r paste(params$iso3, ":", params$season, params$year, "Season Outlook")`
#### Generated `r Sys.Date()` from `r params$dataset` data issued `r paste(month.abb[params$issued_month], params$year)`

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

library(kableExtra)
library(knitr)
library(gghdx)
library(cumulus)
library(AzureStor)
library(stringr)
source("R/load_data.R")
source("R/process_data.R")
source("R/plot_data.R")
source("R/cogs.R")
knitr::opts_chunk$set(echo = TRUE) # do not print code by default
knitr::opts_chunk$set(include = FALSE) # do not print output by default
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

gghdx()
```

``` {r constants, echo = FALSE}

if (params$season == "MAM") {
  months_to_include <- c(3, 4, 5)
} else if (params$season == "OND") {
  months_to_include <- c(10, 11, 12)
}

# For Ethiopia, we're looking at PiN data
if (params$iso3 == "ETH") {
  gdf_pop <- get_eth_gdf_pin(params$year) # This contains all adms in ETH
  if (params$adm_level == 2) {
    sel_zones <- subset_adm2_ond_mam()
  } else {
    sel_zones <- NULL
  }
  pcode_col <- glue("admin{params$adm_level}Pcode")
  pop_var <- "PiN"

# While for Somalia we're looking at population
} else if (params$iso3 == "SOM") {
  gdf_pop <- get_som_gdf_pop(params$adm_level)
  sel_zones <- NULL
  pcode_col <- glue("ADM{params$adm_level}_PCODE")
  pop_var <- "Population"
}

```

First load in the historical rainfall data from the Postgres database. We'll process that data to filter to only the months of interest (depending on the season we're looking at), 
and calculate the total precipitation for that season, per pcode and year. We'll then calculate return periods and label tercile/quartiles for each pcode and year. 

```{r loading}

df_rainfall <- get_historical_rainfall(params$dataset, params$iso3, params$adm_level, sel_zones)

if (params$dataset == "seas5") {
  df_rainfall_proc <- process_seas5_rainfall(
    df_rainfall,
    sel_month = params$issued_month,
    months_to_include = months_to_include
  )
} else if (params$dataset == "era5") {
  df_rainfall_proc <- process_era5_rainfall(
    df_rainfall,
    months_to_include = months_to_include
  )
}

df_summary <- join_pop_rainfall(gdf_pop, df_rainfall_proc, pcode_col)
```

``` {r load_cog_stack}

proj_container <- cumulus::blob_containers("dev")$projects
rast_container <- cumulus::blob_containers("prod")$raster
tmp_dir <- "tmp/"
azure_dir <- "ds-seasonal-bulletin/"
src_cog_dir <- paste0(params$dataset, "/monthly/processed")

# First check if the COGs (average and current) exist already
f_avg <- cog_filename(params$iso3, "avg", params$dataset, params$season, params$issued_month)
azure_filepath_avg <- paste0(azure_dir, params$iso3, "/", f_avg)
local_filepath_avg <- paste0(tmp_dir, f_avg)
exists_avg <- cog_exists(proj_container, azure_filepath_avg)

f_cur <- cog_filename(params$iso3, "cur", params$dataset, params$season, params$issued_month)
azure_filepath_cur <- paste0(azure_dir, params$iso3, "/", f_cur)
local_filepath_cur <- paste0(tmp_dir, f_cur)
exists_cur <- cog_exists(proj_container, azure_filepath_cur)

if (exists_avg && exists_cur) {
  r_avg <- load_cog_from_azure(
    proj_container,
    azure_filepath_avg,
    local_filepath_avg
  )
  r_cur <- load_cog_from_azure(
    proj_container,
    azure_filepath_cur,
    local_filepath_cur
  )
} else {
  cog_df <- get_cogs_to_process(
    rast_container,
    params$dataset,
    months_to_include,
    params$issued_month
  )

  # TODO: Improve this processing -- see Zack suggestions
  # Load by year, take the sum, then stack
  yearly_sums <- list()
  for (sel_year in unique(cog_df$year)) {
    yearly_cogs <- filter(cog_df, year == sel_year)
    urls <- paste0("/vsiaz/raster/", yearly_cogs$name)
    cogs <- rast(urls)
    cogs_clipped <- crop(cogs, gdf_pop)
    cogs_masked <- mask(cogs_clipped, gdf_pop)
    # go from mm/day to mm/month
    yearly_sum <- sum((cogs_masked * 30))
    if (sel_year == max(cog_df$year)) {
      r_cur <- yearly_sum
    }
    yearly_sums <- append(yearly_sums, yearly_sum)
  }

  r_avg <- mean(yearly_sums)

  # Now save to Azure
  upload_cog_to_azure(
    r_cur,
    proj_container,
    local_filepath_cur,
    azure_filepath_cur
  )
  upload_cog_to_azure(
    r_avg,
    proj_container,
    local_filepath_avg,
    azure_filepath_avg
  )
}

```

``` {r plot_anomalies, include = TRUE, fig.width=10, fig.height=5}

# If applicable, subset the gdf by only the selected zones
if (!is.null(sel_zones)) {
  gdf_sel <- gdf_pop %>% filter(!!sym(pcode_col) %in% sel_zones)
} else {
  gdf_sel <- gdf_pop
}

plot_anomalies(r_avg, r_cur, gdf_sel)
```


``` {r summarize}

# TODO: Also the `total_unique_pcodes` value here doesn't seem correct...
df_annual_pop <- calc_yearly_impact(df_summary) %>%
  mutate(
    rank = rank(-total_affected, ties.method = "average"),
    exceedance_prob = rank / (n() + 1),
    return_period = 1 / exceedance_prob
  )

df_sel <- df_annual_pop %>%
  filter(year == params$year)

total_affected <- df_sel$total_affected
total_rp <- df_sel$return_period

```

`r format(round(total_affected), big.mark=",")` people in need are forecasted to experience lower tercile rainfall throughout the upcoming season.

We see this level of people in need once every `r round(total_rp, 1)` years.


``` {r rp_plot, include = TRUE, fig.height=5}

affected_zones <- str_split(df_sel$affected_zones, ", ")[[1]]
df_summary_year <- df_summary %>% filter(year == params$year)

# Join again with all adms
gdf_rp <- gdf_pop %>%
  left_join(df_summary_year, by = setNames("pcode", pcode_col))

# Get all pcodes in the lower quantile
gdf_lower_quantile <- gdf_pop %>%
  filter(!!sym(pcode_col) %in% affected_zones)

plot_return_periods(gdf_rp, gdf_lower_quantile)
```


``` {r calc_yearly_avg}
df_rainfall_annual <- df_summary %>%
  group_by(year) %>%
  summarise(total_rainfall = sum(total_rainfall, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(total_rainfall) %>%
  mutate(year = as.numeric(year))

df_pop_annual <- df_summary %>%
  group_by(year) %>%
  summarise(total_pop = sum(TotalPop_tercile, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(total_pop) %>%
  mutate(year = as.numeric(year))
```


``` {r, include = TRUE, fig.width=10, fig.height=5}

# These years will be highlighted on the chart
REFERENCE_YEARS <- c(2023, 2022, 2020, 2014, 2024)

df_annual_summary <- df_pop_annual %>%
  full_join(df_rainfall_annual, by = "year") %>%
  mutate(point_color = case_when(
    year==params$year~hdx_hex("tomato-hdx"),
    year %in% REFERENCE_YEARS ~ hdx_hex("sapphire-light"),
    TRUE ~ hdx_hex("sapphire-hdx"))) %>%
  filter(year > 2000) # filtering to more recent years -- greater than 2000

plot_annual_scatter(df_annual_summary, REFERENCE_YEARS, params$year, pop_var)

```